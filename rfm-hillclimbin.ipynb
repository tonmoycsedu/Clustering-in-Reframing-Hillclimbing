{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to write in csv\n",
    "import csv\n",
    "def write_list_in_file(final, name):\n",
    "    with open(name, \"w\", newline=\"\",encoding=\"utf8\") as fp:\n",
    "        a = csv.writer(fp, delimiter=',')\n",
    "        a.writerows(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read csv files\n",
    "from csv import reader\n",
    "# Load a CSV file\\n\",\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype <U9 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#load base data\n",
    "base_data = load_csv('season1.csv')    \n",
    "base_data = np.array(base_data[1:])\n",
    "base_size = len(base_data)\n",
    "\n",
    "#no of target elements and load target data\n",
    "elements = [10,20,30,40,50]\n",
    "target_data = load_csv('season4.csv')\n",
    "target_data = np.array(target_data[1:])\n",
    "\n",
    "# #normalize data using a min max normalizer\n",
    "# mixed = np.concatenate((base_data,target_data))\n",
    "# scaler = MinMaxScaler()\n",
    "# mixed_scaled = scaler.fit_transform(mixed)\n",
    "\n",
    "#base_data = scaler.fit_transform(base_data)\n",
    "\n",
    "#normalized base and target data\n",
    "scaler = preprocessing.StandardScaler().fit(base_data)\n",
    "base_scaled = scaler.transform(base_data)\n",
    "target_scaled = scaler.transform(target_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of attributes\n",
    "num_attr = base_scaled[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum Number of Clusters:  2\n"
     ]
    }
   ],
   "source": [
    "#learn number of clusters using silhouette_score\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "num_clusters = 0\n",
    "max_silhouette = -100\n",
    "\n",
    "for n_clusters in range(2,10):\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = clusterer.fit_predict(base_scaled)\n",
    "\n",
    "    silhouette_avg = silhouette_score(base_scaled, cluster_labels)\n",
    "    #print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", silhouette_avg)\n",
    "    \n",
    "    if(silhouette_avg > max_silhouette):\n",
    "        max_silhouette = silhouette_avg\n",
    "        num_clusters = n_clusters\n",
    "\n",
    "print(\"Optimum Number of Clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn centers from source/base model\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(base_scaled)\n",
    "\n",
    "base_centroids = kmeans.cluster_centers_\n",
    "#labels = kmeans.labels_\n",
    "    \n",
    "#print(kmeans.cluster_centers_)\n",
    "#print(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_centroids(alpha,beta):\n",
    "\n",
    "    target_centroids = []\n",
    "    for i in range(num_clusters):\n",
    "        val = []\n",
    "        for j in range(num_attr):\n",
    "            val.append(alpha*base_centroids[i][j] + beta)\n",
    "        \n",
    "        target_centroids.append(val)\n",
    "             \n",
    "\n",
    "    target_centroids = np.array(target_centroids)\n",
    "    #print(\"new centroids: \", target_centroids)\n",
    "    return target_centroids\n",
    "    #print(target_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
