{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to write in csv\n",
    "import csv\n",
    "def write_list_in_file(final, name):\n",
    "    with open(name, \"w\", newline=\"\",encoding=\"utf8\") as fp:\n",
    "        a = csv.writer(fp, delimiter=',')\n",
    "        a.writerows(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to read csv files\n",
    "from csv import reader\n",
    "# Load a CSV file\\n\",\n",
    "def load_csv(filename):\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonmoy/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype <U9 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#load base data\n",
    "base_data = load_csv('season1.csv')    \n",
    "base_data = np.array(base_data[1:])\n",
    "base_size = len(base_data)\n",
    "\n",
    "#no of target elements and load target data\n",
    "elements = [10,20,30,40,50]\n",
    "target_data = load_csv('season4.csv')\n",
    "target_data = np.array(target_data[1:])\n",
    "\n",
    "# #normalize data using a min max normalizer\n",
    "# mixed = np.concatenate((base_data,target_data))\n",
    "# scaler = MinMaxScaler()\n",
    "# mixed_scaled = scaler.fit_transform(mixed)\n",
    "\n",
    "#base_data = scaler.fit_transform(base_data)\n",
    "\n",
    "#normalized base and target data\n",
    "scaler = preprocessing.StandardScaler().fit(base_data)\n",
    "base_scaled = scaler.transform(base_data)\n",
    "target_scaled = scaler.transform(target_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of attributes\n",
    "num_attr = base_scaled[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum Number of Clusters:  2\n"
     ]
    }
   ],
   "source": [
    "#learn number of clusters using silhouette_score\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "num_clusters = 0\n",
    "max_silhouette = -100\n",
    "\n",
    "for n_clusters in range(2,10):\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = clusterer.fit_predict(base_scaled)\n",
    "\n",
    "    silhouette_avg = silhouette_score(base_scaled, cluster_labels)\n",
    "    #print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", silhouette_avg)\n",
    "    \n",
    "    if(silhouette_avg > max_silhouette):\n",
    "        max_silhouette = silhouette_avg\n",
    "        num_clusters = n_clusters\n",
    "\n",
    "print(\"Optimum Number of Clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#learn centers from source/base model\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(base_scaled)\n",
    "\n",
    "base_centroids = kmeans.cluster_centers_\n",
    "#labels = kmeans.labels_\n",
    "    \n",
    "#print(kmeans.cluster_centers_)\n",
    "#print(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applying base model in target result:  422.912806318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonmoy/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:889: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "#base model results\n",
    "kmeans = KMeans(n_clusters=num_clusters, init=base_centroids, max_iter=1)\n",
    "kmeans.fit(target_scaled)\n",
    "print(\"applying base model in target result: \", kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining results on  10  data  396.243217504\n",
      "Retraining results on  20  data  445.42475718\n",
      "Retraining results on  30  data  446.59106654\n",
      "Retraining results on  40  data  395.354560891\n",
      "Retraining results on  50  data  393.615917857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonmoy/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:889: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "#retraining results\n",
    "\n",
    "\n",
    "for n in elements:\n",
    "\n",
    "    #Read Available Limited Target Data\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(target_scaled[:n])\n",
    "    \n",
    "    #Result on whole Target Data\n",
    "    kmeans = KMeans(n_clusters=num_clusters, init=kmeans.cluster_centers_, max_iter=1)\n",
    "    kmeans.fit(target_scaled)\n",
    "\n",
    "    print(\"Retraining results on \", n,\" data \",kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_centroids(alpha,beta):\n",
    "\n",
    "    target_centroids = []\n",
    "    for i in range(num_clusters):\n",
    "        val = []\n",
    "        for j in range(num_attr):\n",
    "            val.append(alpha[j]*base_centroids[i][j] + beta[j])\n",
    "        \n",
    "        target_centroids.append(val)\n",
    "             \n",
    "\n",
    "    target_centroids = np.array(target_centroids)\n",
    "    #print(\"new centroids: \", target_centroids)\n",
    "    return target_centroids\n",
    "    #print(target_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans_custom(k,dataItems,centroids,maxIter,num_attr):\n",
    "    \n",
    "    #print(k,dataItems,centroids,maxInter,num_attr)\n",
    "    old_centroids = []\n",
    "    groups = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        groups.append([])\n",
    "        \n",
    "    iter = 0\n",
    "    while(iter < maxIter):\n",
    "        old_centroids = centroids\n",
    "        for item in dataItems:\n",
    "            row = []\n",
    "            for centroid in centroids:\n",
    "                diff = 0\n",
    "                for i in range(num_attr):\n",
    "                    diff += abs(item[i] - centroid[i])\n",
    "                    \n",
    "                row.append(diff)\n",
    "                \n",
    "            idx = row.index(min(row))\n",
    "            groups[idx].append(item)\n",
    "                    \n",
    "            \n",
    "        iter += 1\n",
    "        \n",
    "    ss = 0    \n",
    "    for i in range(k):\n",
    "        for item in groups[i]:\n",
    "            for j in range(num_attr):\n",
    "            \n",
    "                diff = abs(item[j]- old_centroids[i][j])\n",
    "                ss += pow(diff,2)\n",
    "            \n",
    "        \n",
    "    return (groups,ss/2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_gradient(clusters,reframed_centroids,old_alpha,old_beta):\n",
    "    \n",
    "    gradient_alpha = []\n",
    "    gradient_beta = []\n",
    "    gradient = 0\n",
    "    \n",
    "    for i in range(num_attr):\n",
    "        gradient_alpha.append(0)\n",
    "        gradient_beta.append(0)\n",
    "        \n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "                \n",
    "        for member in clusters[i]:\n",
    "                      \n",
    "            for j in range(num_attr):\n",
    "                \n",
    "                gradient =  (reframed_centroids[i][j] - member[j])\n",
    "                gradient_alpha[j] = gradient_alpha[j] + gradient*reframed_centroids[i][j]\n",
    "                gradient_beta[j] = gradient_beta[j] + gradient\n",
    "\n",
    "    #print(\"******\")\n",
    "    #print(gradient_alpha,gradient_beta)\n",
    "    \n",
    "    new_alpha = []\n",
    "    new_beta = []\n",
    "    \n",
    "    for i in range(num_attr):       \n",
    "        new_alpha.append(old_alpha[i]-.0001*gradient_alpha[i])\n",
    "        new_beta.append(old_beta[i]-.0001*gradient_beta[i])\n",
    "        \n",
    "    return [new_alpha,new_beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_parameters(alpha,beta):\n",
    "    reframed_centroids =  cal_centroids(alpha,beta)\n",
    "    km = kmeans_custom(num_clusters, target_scaled, reframed_centroids, 1, num_attr)\n",
    "    #kmeans.fit(target_scaled)\n",
    "    best_error = km[1]\n",
    "    #centroids = kmeans.cluster_centers_\n",
    "    #labels = kmeans.labels_\n",
    "\n",
    "    count = 0\n",
    "    best_alpha = alpha\n",
    "    best_beta = beta\n",
    "    while(1):\n",
    "        #print(centroids)\n",
    "        #print(\"best error: \",best_error)\n",
    "\n",
    "        #clusters = find_members(centroids,labels)\n",
    "\n",
    "        #reframed_centroids = closest_centroids(reframed_centroids,centroids)\n",
    "\n",
    "        new_alphabeta = cal_gradient(km[0],reframed_centroids,alpha,beta)\n",
    "\n",
    "        alpha = new_alphabeta[0]\n",
    "        beta = new_alphabeta[1]\n",
    "\n",
    "        #print(\"new alpha beta\", alpha, beta)\n",
    "\n",
    "        reframed_centroids =  cal_centroids(alpha,beta)\n",
    "\n",
    "        km = kmeans_custom(num_clusters, target_scaled, reframed_centroids, 1, num_attr)\n",
    "        #kmeans.fit(target_scaled)\n",
    "        new_error = km[1]\n",
    "\n",
    "        #print(\"compare \",best_error,new_error)\n",
    "        if(new_error < best_error):\n",
    "            best_alpha = alpha\n",
    "            best_beta = beta\n",
    "            best_error = new_error\n",
    "            count = 0\n",
    "\n",
    "        elif(new_error == best_error):\n",
    "            if(count<5):\n",
    "                count += 1\n",
    "                continue\n",
    "            else:\n",
    "                break;\n",
    "\n",
    "        else:\n",
    "            break; \n",
    "\n",
    "        #base_centroids = kmeans.cluster_centers_\n",
    "        #labels = kmeans.labels_\n",
    "\n",
    "        #print(kmeans.cluster_centers_)\n",
    "        #print(old_error,new_error)\n",
    "\n",
    "    #print(\"finalparameters\", best_alpha,best_beta)\n",
    "    \n",
    "    target_cent = cal_centroids(best_alpha,best_beta)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, init=target_cent, max_iter=1)\n",
    "    kmeans.fit(target_scaled)\n",
    "    #print(\"Reframing results on \",kmeans.inertia_)\n",
    "    return kmeans.inertia_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonmoy/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:889: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reframing results on  10  data:  377.439384978\n",
      "Reframing results on  20  data:  377.439384978\n",
      "Reframing results on  30  data:  377.439384978\n",
      "Reframing results on  40  data:  378.317661096\n",
      "Reframing results on  50  data:  378.317661096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for n in elements:\n",
    "    \n",
    "    alpha = []\n",
    "    beta = []\n",
    "    avg_m = []\n",
    "    avg_d = []\n",
    "    sum_d = 0\n",
    "    for i in range(num_attr):\n",
    "        sum_m = 0\n",
    "        for j in range(num_clusters):\n",
    "            sum_m = sum_m + base_centroids[j][i]\n",
    "\n",
    "        avg_m.append(sum_m/num_clusters)\n",
    "\n",
    "\n",
    "    for i in range(num_attr):\n",
    "        sum_d = 0\n",
    "        for j in range(len(target_scaled[:n])):\n",
    "            sum_d = sum_d + target_scaled[j][i]\n",
    "\n",
    "        avg_d.append(sum_d/len(target_scaled[:n]))\n",
    "\n",
    "\n",
    "    for i in range(num_attr):\n",
    "        alpha.append(avg_m[i]/avg_d[i])\n",
    "        beta.append(0)\n",
    "\n",
    "    #print(alpha,beta)\n",
    "    #print(\"#####\")\n",
    "    result = learn_parameters(alpha,beta)\n",
    "    print(\"Reframing results on \",n,\" data: \",result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
