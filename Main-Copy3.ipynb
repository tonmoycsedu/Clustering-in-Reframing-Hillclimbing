{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "cats = ['comp.graphics', 'comp.os.ms-windows.misc']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'), categories=cats)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'), categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_data = newsgroups_test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "base_scaled = vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_scaled = vectorizer.transform(target_data)\n",
    "target_scaled = np.asarray(target_scaled.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41342"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_scaled[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ac7ce475ce84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# seed of 10 for reproducibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mclusterer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mcluster_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclusterer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msilhouette_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonmoy/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \"\"\"\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonmoy/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 return_n_iter=True)\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonmoy/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[0;34m(X, n_clusters, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[1;32m    343\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                 \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 x_squared_norms=x_squared_norms, random_state=random_state)\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tonmoy/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             centers = _k_means._centers_sparse(X, labels, n_clusters,\n\u001b[0;32m--> 496\u001b[0;31m                                                distances)\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_k_means\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_centers_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#learn number of clusters using silhouette_score\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "num_clusters = 0\n",
    "max_silhouette = -100\n",
    "\n",
    "for n_clusters in range(2,10):\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters,init='k-means++',max_iter=1000)\n",
    "    cluster_labels = clusterer.fit_predict(base_scaled)\n",
    "\n",
    "    silhouette_avg = silhouette_score(base_scaled, cluster_labels)\n",
    "    #print(\"For n_clusters =\", n_clusters,\"The average silhouette_scinit='k-means++',ore is :\", silhouette_avg)\n",
    "    \n",
    "    if(silhouette_avg > max_silhouette):\n",
    "        max_silhouette = silhouette_avg\n",
    "        num_clusters = n_clusters\n",
    "\n",
    "print(\"Optimum Number of Clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1109.641885059994\n"
     ]
    }
   ],
   "source": [
    "#learn centers from source/base model\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters,init='k-means++')\n",
    "kmeans.fit(base_scaled)\n",
    "\n",
    "base_centroids = kmeans.cluster_centers_\n",
    "#labels = kmeans.labels_\n",
    "    \n",
    "#print(kmeans.cluster_centers_)\n",
    "print(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###spectral clustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "clustering = SpectralClustering(n_clusters=2,\n",
    "         assign_labels=\"discretize\",\n",
    "         random_state=0,affinity='nearest_neighbors',n_neighbors=27).fit(target_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_attr = base_centroids[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1175, 41342)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_centroids(alpha,beta):\n",
    "\n",
    "    target_centroids = []\n",
    "    for i in range(num_clusters):\n",
    "        val = []\n",
    "        for j in range(num_attr):\n",
    "            val.append(alpha[j]*base_centroids[i][j] + beta[j])\n",
    "        \n",
    "        target_centroids.append(val)\n",
    "             \n",
    "\n",
    "    target_centroids = np.array(target_centroids)\n",
    "    #print(\"new centroids: \", target_centroids)\n",
    "    return target_centroids\n",
    "    #print(target_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans_custom(k,dataItems,centroids,maxIter,num_attr):\n",
    "    \n",
    "    #print(k,dataItems,centroids,maxInter,num_attr)\n",
    "    old_centroids = []\n",
    "    groups = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        groups.append([])\n",
    "        \n",
    "    iter = 0\n",
    "    while(iter < maxIter):\n",
    "        old_centroids = centroids\n",
    "        for item in dataItems:\n",
    "            row = []\n",
    "            for centroid in centroids:\n",
    "                diff = 0\n",
    "                for i in range(num_attr):\n",
    "                    diff += abs(item[i] - centroid[i])\n",
    "                    \n",
    "                row.append(diff)\n",
    "                \n",
    "            idx = row.index(min(row))\n",
    "            groups[idx].append(item)\n",
    "                    \n",
    "            \n",
    "        iter += 1\n",
    "        \n",
    "    ss = 0    \n",
    "    for i in range(k):\n",
    "        for item in groups[i]:\n",
    "            for j in range(num_attr):\n",
    "            \n",
    "                diff = abs(item[j]- old_centroids[i][j])\n",
    "                ss += pow(diff,2)\n",
    "            \n",
    "        \n",
    "    return (groups,ss/2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_gradient(clusters,reframed_centroids,old_alpha,old_beta):\n",
    "    \n",
    "    gradient_alpha = []\n",
    "    gradient_beta = []\n",
    "    gradient = 0\n",
    "    \n",
    "    for i in range(num_attr):\n",
    "        gradient_alpha.append(0)\n",
    "        gradient_beta.append(0)\n",
    "        \n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "                \n",
    "        for member in clusters[i]:\n",
    "                      \n",
    "            for j in range(num_attr):\n",
    "                \n",
    "                gradient =  (reframed_centroids[i][j] - member[j])\n",
    "                gradient_alpha[j] = gradient_alpha[j] + gradient*base_centroids[i][j]\n",
    "                gradient_beta[j] = gradient_beta[j] + gradient\n",
    "\n",
    "    #print(\"******\")\n",
    "    #print(gradient_alpha,gradient_beta)\n",
    "    \n",
    "    new_alpha = []\n",
    "    new_beta = []\n",
    "    \n",
    "    for i in range(num_attr):       \n",
    "        new_alpha.append(old_alpha[i]-.1*gradient_alpha[i])\n",
    "        new_beta.append(old_beta[i]-.1*gradient_beta[i])\n",
    "        \n",
    "    return [new_alpha,new_beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_parameters(alpha,beta,target_avail):\n",
    "    reframed_centroids =  cal_centroids(alpha,beta)\n",
    "    km = kmeans_custom(num_clusters, target_avail, reframed_centroids, 1, num_attr)\n",
    "    #kmeans.fit(target_scaled)\n",
    "    best_error = round(km[1],5)\n",
    "    #centroids = kmeans.cluster_centers_\n",
    "    #labels = kmeans.labels_\n",
    "\n",
    "    count = 0\n",
    "    best_alpha = alpha\n",
    "    best_beta = beta\n",
    "    while(1):\n",
    "        #print(centroids)\n",
    "        #print(\"best error: \",best_error)\n",
    "\n",
    "        #clusters = find_members(centroids,labels)\n",
    "\n",
    "        #reframed_centroids = closest_centroids(reframed_centroids,centroids)\n",
    "\n",
    "        new_alphabeta = cal_gradient(km[0],reframed_centroids,alpha,beta)\n",
    "\n",
    "        alpha = new_alphabeta[0]\n",
    "        beta = new_alphabeta[1]\n",
    "\n",
    "        #print(\"new alpha beta\", alpha, beta)\n",
    "\n",
    "        reframed_centroids =  cal_centroids(alpha,beta)\n",
    "\n",
    "        km = kmeans_custom(num_clusters, target_avail, reframed_centroids, 1, num_attr)\n",
    "        #kmeans.fit(target_scaled)\n",
    "        new_error = round(km[1],5)\n",
    "\n",
    "        print(\"compare \",best_error,new_error)\n",
    "        if(new_error < best_error):\n",
    "            best_alpha = alpha\n",
    "            best_beta = beta\n",
    "            best_error = new_error\n",
    "            count = 0\n",
    "\n",
    "        elif(new_error == best_error):\n",
    "            if(count<5):\n",
    "                count += 1\n",
    "                continue\n",
    "            else:\n",
    "                break;\n",
    "\n",
    "        else:\n",
    "            break; \n",
    "\n",
    "        #base_centroids = kmeans.cluster_centers_\n",
    "        #labels = kmeans.labels_\n",
    "\n",
    "        #print(kmeans.cluster_centers_)\n",
    "        #print(old_error,new_error)\n",
    "\n",
    "    #print(\"finalparameters\", best_alpha,best_beta)\n",
    "    \n",
    "    target_cent = cal_centroids(best_alpha,best_beta)\n",
    "    #km = kmeans_custom(num_clusters, target_scaled, target_cent, 1, num_attr)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, init=target_cent, max_iter=1)\n",
    "    kmeans.fit(target_scaled)\n",
    "    #print(\"Reframing results on \",kmeans.inertia_)\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_scores(class_labels, clustering_result):\n",
    "    \n",
    "    cluster_labels = clustering_result.labels_\n",
    "    \n",
    "    print(\"Reframing results on \",n,\" data: \")\n",
    "    \n",
    "    print(\"SSE: \", clustering_result.inertia_)\n",
    "    \n",
    "    conf_mat = confusion_matrix(class_labels, cluster_labels)\n",
    "    m=0\n",
    "    for i in range(num_clusters):\n",
    "        m = m+np.max(conf_mat[i])\n",
    "    \n",
    "    print(\"purity: \", m/len(class_labels))  \n",
    "    \n",
    "    NMI = normalized_mutual_info_score(class_labels, cluster_labels)\n",
    "    print(\"NMI: \",NMI)\n",
    "    \n",
    "    RI = accuracy_score(class_labels, cluster_labels)\n",
    "    print(\"RI: \",RI)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compare  12.80283 12.88368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonmoy/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:889: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reframing results on  30  data: \n",
      "SSE:  741.089706982\n",
      "purity:  0.78288633461\n",
      "NMI:  0.252018824908\n",
      "RI:  0.78288633461\n",
      "compare  22.51795 22.62286\n",
      "Reframing results on  50  data: \n",
      "SSE:  740.650269441\n",
      "purity:  0.761174968072\n",
      "NMI:  0.250731263657\n",
      "RI:  0.761174968072\n",
      "compare  32.69472 43.57696\n",
      "Reframing results on  70  data: \n",
      "SSE:  741.151283138\n",
      "purity:  0.79054916986\n",
      "NMI:  0.262857182661\n",
      "RI:  0.79054916986\n",
      "compare  42.04126 66.40617\n",
      "Reframing results on  90  data: \n",
      "SSE:  740.786720794\n",
      "purity:  0.789272030651\n",
      "NMI:  0.277758740409\n",
      "RI:  0.789272030651\n",
      "compare  55.31959 106.60894\n",
      "Reframing results on  120  data: \n",
      "SSE:  740.483569126\n",
      "purity:  0.779054916986\n",
      "NMI:  0.287250342007\n",
      "RI:  0.779054916986\n",
      "compare  65.14086 144.02664\n",
      "Reframing results on  140  data: \n",
      "SSE:  740.403157874\n",
      "purity:  0.777777777778\n",
      "NMI:  0.29160363874\n",
      "RI:  0.777777777778\n",
      "compare  75.13099 202.1769\n",
      "Reframing results on  160  data: \n",
      "SSE:  740.543104911\n",
      "purity:  0.787994891443\n",
      "NMI:  0.297655105559\n",
      "RI:  0.787994891443\n",
      "compare  84.99192 258.53128\n",
      "Reframing results on  180  data: \n",
      "SSE:  740.573784627\n",
      "purity:  0.79054916986\n",
      "NMI:  0.299998302042\n",
      "RI:  0.79054916986\n",
      "compare  94.36872 322.7106\n",
      "Reframing results on  200  data: \n",
      "SSE:  740.593262239\n",
      "purity:  0.791826309068\n",
      "NMI:  0.302102549376\n",
      "RI:  0.791826309068\n"
     ]
    }
   ],
   "source": [
    "elements = [30,50,70,90,120,140,160,180,200]\n",
    "for n in elements:\n",
    "    alpha = []\n",
    "    beta = []\n",
    "    avg_m = []\n",
    "    avg_d = []\n",
    "    sum_d = 0\n",
    "    for i in range(num_attr):\n",
    "        sum_m = 0\n",
    "        for j in range(num_clusters):\n",
    "            sum_m = sum_m + base_centroids[j][i]\n",
    "        #print(sum_m)\n",
    "        avg_m.append(sum_m/num_clusters)\n",
    "    \n",
    "    target_avail = vectorizer.transform(target_data[:n])\n",
    "    target_avail = np.asarray(target_avail.todense())\n",
    "    for i in range(num_attr):\n",
    "        sum_d = 0\n",
    "        for j in range(len(target_avail)):\n",
    "            sum_d = sum_d + target_avail[j][i]\n",
    "        #print(sum_d)\n",
    "        avg_d.append(sum_d/len(target_avail))\n",
    "\n",
    "\n",
    "    for i in range(num_attr):\n",
    "        alpha.append(avg_d[i]/avg_m[i])\n",
    "        beta.append(0)\n",
    "    \n",
    "    #print(len(alpha))\n",
    "    #print(alpha,beta)\n",
    "    #print(\"#####\")\n",
    "    result = learn_parameters(alpha,beta,target_avail)\n",
    "    find_scores(newsgroups_test.target,result)\n",
    "    #print(\"NMI: \",normalized_mutual_info_score(newsgroups_test.target, result.labels_))\n",
    "    \n",
    "    #print(\"Reframing results on \",n,\" data: \",result.inertia_)\n",
    "    #print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nj = np.zeros(2) ##for class\n",
    "nl = np.zeros(2) ##for cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nj[1] = np.count_nonzero(newsgroups_test.target)\n",
    "nj[0] = len(newsgroups_test.target) - nj[1]\n",
    "\n",
    "nl[1] = np.count_nonzero(result.labels_)\n",
    "nl[0] = len(result.labels_) - nl[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 389.  394.] [ 378.  405.]\n"
     ]
    }
   ],
   "source": [
    "print(nj,nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "njl = np.zeros((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 286.,  103.],\n",
       "       [  92.,  302.]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(newsgroups_test.target)):\n",
    "    x = newsgroups_test.target[i]\n",
    "    y = result.labels_[i]\n",
    "    njl[x][y] = njl[x][y]+1\n",
    "    \n",
    "njl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_xy = 0\n",
    "sum_x = 0\n",
    "sum_y = 0\n",
    "flag = 0\n",
    "N = len(newsgroups_test.target)\n",
    "for x in range(2):\n",
    "    \n",
    "    sum_x = sum_x+ nj[x] * np.log10(nj[x]/N)\n",
    "    \n",
    "    for y in range(2):\n",
    "        \n",
    "        sum_xy = sum_xy+ njl[x][y] * np.log10((N*njl[x][y])/(nj[x]*nl[y]))\n",
    "        \n",
    "        if(flag==0):\n",
    "            sum_y = sum_y+ nl[y] * np.log10(nl[y]/N)\n",
    "            flag = 1\n",
    "    flag = 0\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.861865144370839"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18897661896689369"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NMI = sum_xy/np.sqrt(sum_x*sum_y)\n",
    "NMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
