{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "cats = ['comp.graphics', 'comp.os.ms-windows.misc']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers', 'footers', 'quotes'), categories=cats)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'), categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsgroups_train = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_data = newsgroups_test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "base_scaled = vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scaled = vectorizer.transform(target_data)\n",
    "target_scaled = np.asarray(target_scaled.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41342"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_scaled[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimum Number of Clusters:  3\n"
     ]
    }
   ],
   "source": [
    "#learn number of clusters using silhouette_score\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "num_clusters = 0\n",
    "max_silhouette = -100\n",
    "\n",
    "for n_clusters in range(2,10):\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters,init='k-means++',max_iter=1000)\n",
    "    cluster_labels = clusterer.fit_predict(base_scaled)\n",
    "\n",
    "    silhouette_avg = silhouette_score(base_scaled, cluster_labels)\n",
    "    #print(\"For n_clusters =\", n_clusters,\"The average silhouette_scinit='k-means++',ore is :\", silhouette_avg)\n",
    "    \n",
    "    if(silhouette_avg > max_silhouette):\n",
    "        max_silhouette = silhouette_avg\n",
    "        num_clusters = n_clusters\n",
    "\n",
    "print(\"Optimum Number of Clusters: \",num_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1105.5843047778483\n"
     ]
    }
   ],
   "source": [
    "#learn centers from source/base model\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters,init='k-means++',max_iter=1000)\n",
    "kmeans.fit(base_scaled)\n",
    "\n",
    "base_centroids = kmeans.cluster_centers_\n",
    "#labels = kmeans.labels_\n",
    "    \n",
    "#print(kmeans.cluster_centers_)\n",
    "print(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_attr = base_centroids[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1175, 41342)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_centroids(alpha,beta):\n",
    "\n",
    "    target_centroids = []\n",
    "    for i in range(num_clusters):\n",
    "        val = []\n",
    "        for j in range(num_attr):\n",
    "            val.append(alpha[j]*base_centroids[i][j] + beta[j])\n",
    "        \n",
    "        target_centroids.append(val)\n",
    "             \n",
    "\n",
    "    target_centroids = np.array(target_centroids)\n",
    "    #print(\"new centroids: \", target_centroids)\n",
    "    return target_centroids\n",
    "    #print(target_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans_custom(k,dataItems,centroids,maxIter,num_attr):\n",
    "    \n",
    "    #print(k,dataItems,centroids,maxInter,num_attr)\n",
    "    old_centroids = []\n",
    "    groups = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        groups.append([])\n",
    "        \n",
    "    iter = 0\n",
    "    while(iter < maxIter):\n",
    "        old_centroids = centroids\n",
    "        for item in dataItems:\n",
    "            row = []\n",
    "            for centroid in centroids:\n",
    "                diff = 0\n",
    "                for i in range(num_attr):\n",
    "                    diff += abs(item[i] - centroid[i])\n",
    "                    \n",
    "                row.append(diff)\n",
    "                \n",
    "            idx = row.index(min(row))\n",
    "            groups[idx].append(item)\n",
    "                    \n",
    "            \n",
    "        iter += 1\n",
    "        \n",
    "    ss = 0    \n",
    "    for i in range(k):\n",
    "        for item in groups[i]:\n",
    "            for j in range(num_attr):\n",
    "            \n",
    "                diff = abs(item[j]- old_centroids[i][j])\n",
    "                ss += pow(diff,2)\n",
    "            \n",
    "        \n",
    "    return (groups,ss/2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_gradient(clusters,reframed_centroids,old_alpha,old_beta):\n",
    "    \n",
    "    gradient_alpha = []\n",
    "    gradient_beta = []\n",
    "    gradient = 0\n",
    "    \n",
    "    for i in range(num_attr):\n",
    "        gradient_alpha.append(0)\n",
    "        gradient_beta.append(0)\n",
    "        \n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "                \n",
    "        for member in clusters[i]:\n",
    "                      \n",
    "            for j in range(num_attr):\n",
    "                \n",
    "                gradient =  (reframed_centroids[i][j] - member[j])\n",
    "                gradient_alpha[j] = gradient_alpha[j] + gradient*base_centroids[i][j]\n",
    "                gradient_beta[j] = gradient_beta[j] + gradient\n",
    "\n",
    "    #print(\"******\")\n",
    "    #print(gradient_alpha,gradient_beta)\n",
    "    \n",
    "    new_alpha = []\n",
    "    new_beta = []\n",
    "    \n",
    "    for i in range(num_attr):       \n",
    "        new_alpha.append(old_alpha[i]-.01*gradient_alpha[i])\n",
    "        new_beta.append(old_beta[i]-.01*gradient_beta[i])\n",
    "        \n",
    "    return [new_alpha,new_beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_parameters(alpha,beta,target_avail):\n",
    "    reframed_centroids =  cal_centroids(alpha,beta)\n",
    "    km = kmeans_custom(num_clusters, target_avail, reframed_centroids, 1, num_attr)\n",
    "    #kmeans.fit(target_scaled)\n",
    "    best_error = round(km[1],5)\n",
    "    #centroids = kmeans.cluster_centers_\n",
    "    #labels = kmeans.labels_\n",
    "\n",
    "    count = 0\n",
    "    best_alpha = alpha\n",
    "    best_beta = beta\n",
    "    while(1):\n",
    "        #print(centroids)\n",
    "        #print(\"best error: \",best_error)\n",
    "\n",
    "        #clusters = find_members(centroids,labels)\n",
    "\n",
    "        #reframed_centroids = closest_centroids(reframed_centroids,centroids)\n",
    "\n",
    "        new_alphabeta = cal_gradient(km[0],reframed_centroids,alpha,beta)\n",
    "\n",
    "        alpha = new_alphabeta[0]\n",
    "        beta = new_alphabeta[1]\n",
    "\n",
    "        #print(\"new alpha beta\", alpha, beta)\n",
    "\n",
    "        reframed_centroids =  cal_centroids(alpha,beta)\n",
    "\n",
    "        km = kmeans_custom(num_clusters, target_avail, reframed_centroids, 1, num_attr)\n",
    "        #kmeans.fit(target_scaled)\n",
    "        new_error = round(km[1],5)\n",
    "\n",
    "        print(\"compare \",best_error,new_error)\n",
    "        if(new_error < best_error):\n",
    "            best_alpha = alpha\n",
    "            best_beta = beta\n",
    "            best_error = new_error\n",
    "            count = 0\n",
    "\n",
    "        elif(new_error == best_error):\n",
    "            if(count<5):\n",
    "                count += 1\n",
    "                continue\n",
    "            else:\n",
    "                break;\n",
    "\n",
    "        else:\n",
    "            break; \n",
    "\n",
    "        #base_centroids = kmeans.cluster_centers_\n",
    "        #labels = kmeans.labels_\n",
    "\n",
    "        #print(kmeans.cluster_centers_)\n",
    "        #print(old_error,new_error)\n",
    "\n",
    "    #print(\"finalparameters\", best_alpha,best_beta)\n",
    "    \n",
    "    target_cent = cal_centroids(best_alpha,best_beta)\n",
    "    #km = kmeans_custom(num_clusters, target_scaled, target_cent, 1, num_attr)\n",
    "    kmeans = KMeans(n_clusters=num_clusters, init=target_cent, max_iter=1)\n",
    "    kmeans.fit(target_scaled)\n",
    "    #print(\"Reframing results on \",kmeans.inertia_)\n",
    "    return kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41342\n",
      "compare  3.97192 3.95445\n",
      "compare  3.95445 3.94031\n",
      "compare  3.94031 3.92885\n",
      "compare  3.92885 3.91957\n",
      "compare  3.91957 3.98671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonmoy/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:889: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reframing results on  10  data:  740.405279352\n"
     ]
    }
   ],
   "source": [
    "elements = [10]\n",
    "for n in elements:\n",
    "    alpha = []\n",
    "    beta = []\n",
    "    avg_m = []\n",
    "    avg_d = []\n",
    "    sum_d = 0\n",
    "    for i in range(num_attr):\n",
    "        sum_m = 0\n",
    "        for j in range(num_clusters):\n",
    "            sum_m = sum_m + base_centroids[j][i]\n",
    "        #print(sum_m)\n",
    "        avg_m.append(sum_m/num_clusters)\n",
    "    \n",
    "    target_avail = vectorizer.transform(target_data[:n])\n",
    "    target_avail = np.asarray(target_avail.todense())\n",
    "    for i in range(num_attr):\n",
    "        sum_d = 0\n",
    "        for j in range(len(target_avail)):\n",
    "            sum_d = sum_d + target_avail[j][i]\n",
    "        #print(sum_d)\n",
    "        avg_d.append(sum_d/len(target_avail))\n",
    "\n",
    "\n",
    "    for i in range(num_attr):\n",
    "        alpha.append(avg_d[i]/avg_m[i])\n",
    "        beta.append(0)\n",
    "    \n",
    "    print(len(alpha))\n",
    "    #print(alpha,beta)\n",
    "    #print(\"#####\")\n",
    "    result = learn_parameters(alpha,beta,target_avail)\n",
    "    print(\"Reframing results on \",n,\" data: \",result)\n",
    "    #print(result)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
